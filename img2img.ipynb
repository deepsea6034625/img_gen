{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install einops ninja\n",
        "#!pip install --upgrade https://download.pytorch.org/whl/nightly/cu111/torch-1.11.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cu111/torchvision-0.12.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import tensorflow\n",
        "import io\n",
        "import os, time\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "yvCwbMICyn4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "5Z-92KL6zZUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional image that will be treated like a text prompt. If it's a zip, we take an average of all the images / experimental\n",
        "# image_prompt_file = \"https://images.pexels.com/photos/614810/pexels-photo-614810.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\" #@param {type: \"string\"}\n",
        "image_prompt_file = \"https://img.freepik.com/free-photo/front-view-beautiful-man_23-2148780802.jpg?w=740&t=st=1704837609~exp=1704838209~hmac=a4133bfc49105a9a3e107708666534aefc6fc78b3c55d44526380a192f6f3405\" #@param {type: \"string\"}\n",
        "images_prompt_weight =  1 #@param {type: \"number\"}\n",
        "\n",
        "# Speed at which to try approximating the text. Too fast seems to give strange results. Maximum is 100.\n",
        "speed = 10  #@param {type: \"number\"}\n",
        "\n",
        "# How many steps to run. Each step generates one frame.\n",
        "steps = 100 #@param {type: \"number\"}\n",
        "\n",
        "# Change the seed to generate variations of the same prompt\n",
        "seed = -1 #@param {type: \"number\"}\n",
        "\n",
        "# We haven't completely understood which parameters influence the generation of this model. Changing the learning rate could help (between 0 and 100)\n",
        "learning_rate = 10\n",
        "\n",
        "model_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl'\n",
        "\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "social = False\n",
        "\n",
        "smoothing = (100.0-speed)/100.0\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define necessary functions\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    if os.path.exists(basename):\n",
        "        return basename\n",
        "    else:\n",
        "        !wget -N '{url_or_path}'\n",
        "        return basename\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "\n",
        "def embed_img_file(path):\n",
        "  image = Image.open(path).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "def embed_img_url(url):\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "    # Open the image using PIL\n",
        "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "  else:\n",
        "    print(f\"Failed to download image. Status code: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "def embed_url(url):\n",
        "  image = Image.open(fetch(url)).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/32\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "\n",
        "clip_model = CLIP()\n",
        "\n",
        "# Load stylegan model\n",
        "\n",
        "network_url = model_url\n",
        "\n",
        "with open(fetch_model(network_url), 'rb') as fp:\n",
        "  G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "# Fix the coordinate grid to w_avg\n",
        "shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n",
        "G.synthesis.input.affine.bias.data.add_(shift.squeeze(0))\n",
        "G.synthesis.input.affine.weight.data.zero_()\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "w_stds = G.mapping(zs, None).std(0)\n",
        "\n",
        "# Run Settings\n",
        "from glob import glob\n",
        "\n",
        "#target = embed_img_file(image_prompt_file)\n",
        "target = embed_img_url(image_prompt_file)\n",
        "prompts = [(images_prompt_weight, target)]\n",
        "\n",
        "# Actually do the run\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "  ])\n",
        "\n",
        "def run():\n",
        "  torch.manual_seed(seed)\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "  # Init\n",
        "  # Method 1: sample 32 inits and choose the one closest to prompt\n",
        "\n",
        "  with torch.no_grad():\n",
        "    qs = []\n",
        "    losses = []\n",
        "    for _ in range(8):\n",
        "      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "      embeds = embed_image(images.add(1).div(2))\n",
        "      loss = 0\n",
        "      for (w, t) in prompts:\n",
        "        loss += w * spherical_dist_loss(embeds, t).mean(0)\n",
        "      i = torch.argmin(loss)\n",
        "      qs.append(q[i])\n",
        "      losses.append(loss[i])\n",
        "    qs = torch.stack(qs)\n",
        "    losses = torch.stack(losses)\n",
        "    print(losses)\n",
        "    print(losses.shape, qs.shape)\n",
        "    i = torch.argmin(losses)\n",
        "    q = qs[i].unsqueeze(0).requires_grad_()\n",
        "\n",
        "  # Method 2: Random init depending only on the seed.\n",
        "\n",
        "  # Sampling loop\n",
        "  q_ema = q\n",
        "  opt = torch.optim.AdamW([q], lr=learning_rate/250.0, betas=(0.0,0.999))\n",
        "  loop = tqdm(range(steps))\n",
        "  for i in loop:\n",
        "    opt.zero_grad()\n",
        "    w = q * w_stds\n",
        "    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
        "    embed = embed_image(image.add(1).div(2))\n",
        "    loss = 0\n",
        "    for (w, t) in prompts:\n",
        "      loss += w * spherical_dist_loss(embed, t).mean(0)\n",
        "    print(loss)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "    q_ema = q_ema * smoothing + q * (1-smoothing)\n",
        "    latent = q_ema * w_stds + G.mapping.w_avg\n",
        "    image = G.synthesis(latent, noise_mode='const')\n",
        "\n",
        "    #if i % 10 == 0:\n",
        "    #  display(TF.to_pil_image(tf(image)[0]))\n",
        "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    os.makedirs(\"/tmp/ffmpeg\", exist_ok=True)\n",
        "    if i % 5 == 0:\n",
        "      pil_image.save(f'{output_path}/output_{i:04}.jpg')\n",
        "      display(pil_image)\n",
        "    pil_image.save(f'/tmp/ffmpeg/output_{i:04}.jpg')\n",
        "  return latent\n",
        "\n",
        "try:\n",
        "  latent = run()\n",
        "  torch.save(latent, f\"{output_path}/latent.pt\")\n",
        "except KeyboardInterrupt:\n",
        "  pass\n",
        "\n",
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "\n",
        "last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "!cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "\n",
        "\n",
        "\n",
        "encoding_options = \"-c:v libx264 -crf 20 -preset slow -vf format=yuv420p -c:a aac -movflags +faststart\"\n",
        "\n",
        "!ffmpeg  -r 10 -i /tmp/ffmpeg/%*.jpg -y {encoding_options} /tmp/vid_no_audio.mp4\n",
        "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg\n",
        "\n",
        "import os.path\n",
        "if not os.path.exists(out_file):\n",
        "  raise Exception(\"Expected output file does not exist.\")"
      ],
      "metadata": {
        "id": "SIng1O5atsEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}